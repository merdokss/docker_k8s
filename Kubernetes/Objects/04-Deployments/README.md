## Deployment

Deployment w Kubernetes to obiekt, ktÃ³ry zarzÄ…dza wdraÅ¼aniem aplikacji na klastrze. Deployment umoÅ¼liwia deklaratywne zarzÄ…dzanie aplikacjami, co oznacza, Å¼e definiujesz poÅ¼Ä…dany stan aplikacji, a Kubernetes automatycznie dÄ…Å¼y do osiÄ…gniÄ™cia tego stanu. Deploymenty sÄ… uÅ¼ywane do:

1. **Tworzenia i skalowania replik podÃ³w**: Deploymenty pozwalajÄ… na Å‚atwe tworzenie i skalowanie liczby replik podÃ³w, co zapewnia wysokÄ… dostÄ™pnoÅ›Ä‡ i skalowalnoÅ›Ä‡ aplikacji.
2. **Aktualizacji aplikacji**: Deploymenty umoÅ¼liwiajÄ… bezpieczne i kontrolowane aktualizacje aplikacji, minimalizujÄ…c przestoje i ryzyko bÅ‚Ä™dÃ³w. Kubernetes wspiera rÃ³Å¼ne strategie aktualizacji, takie jak RollingUpdate i Recreate.
3. **ZarzÄ…dzania rollbackami**: Deploymenty przechowujÄ… historiÄ™ wdroÅ¼eÅ„, co pozwala na Å‚atwe cofanie siÄ™ do poprzednich wersji aplikacji w przypadku problemÃ³w.
4. **Monitorowania stanu aplikacji**: Deploymenty monitorujÄ… stan aplikacji i automatycznie podejmujÄ… dziaÅ‚ania naprawcze, takie jak ponowne uruchamianie podÃ³w w przypadku awarii.

PrzykÅ‚ad definicji Deploymentu:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

W powyÅ¼szym przykÅ‚adzie:

- `replicas: 3` okreÅ›la liczbÄ™ replik podÃ³w, ktÃ³re powinny byÄ‡ uruchomione.
- `selector` definiuje selektor, ktÃ³ry okreÅ›la, ktÃ³re pody powinny byÄ‡ zarzÄ…dzane przez ten Deployment.
- `template` definiuje szablon dla podÃ³w, ktÃ³re bÄ™dÄ… tworzone przez Deployment.
- `containers` definiuje kontenery, ktÃ³re bÄ™dÄ… uruchomione w podach.


### Strategie Deploymentu

W Kubernetes istniejÄ… rÃ³Å¼ne strategie deploymentu, ktÃ³re pozwalajÄ… na kontrolowanie sposobu wdraÅ¼ania nowych wersji aplikacji. Oto najwaÅ¼niejsze z nich:

1. **RollingUpdate**:
   - Jest to domyÅ›lna strategia deploymentu w Kubernetes.
   - Polega na stopniowym zastÄ™powaniu starych replik podÃ³w nowymi, co pozwala na zachowanie wysokiej dostÄ™pnoÅ›ci aplikacji podczas aktualizacji.
   - MoÅ¼na kontrolowaÄ‡ liczbÄ™ jednoczeÅ›nie niedostÄ™pnych podÃ³w (`maxUnavailable`) oraz liczbÄ™ jednoczeÅ›nie tworzonych nowych podÃ³w (`maxSurge`).
   - PrzykÅ‚ad konfiguracji:
     ```yaml
     strategy:
       type: RollingUpdate
       rollingUpdate:
         maxUnavailable: 1
         maxSurge: 1
     ```

2. **Recreate**:
   - W tej strategii wszystkie stare repliki podÃ³w sÄ… najpierw usuwane, a nastÄ™pnie tworzone sÄ… nowe repliki.
   - MoÅ¼e to prowadziÄ‡ do krÃ³tkiego przestoju aplikacji, poniewaÅ¼ nie ma jednoczesnego uruchamiania starych i nowych podÃ³w.
   - PrzykÅ‚ad konfiguracji:
     ```yaml
     strategy:
       type: Recreate
     ```

3. **Blue-Green Deployment**:
   - Ta strategia polega na uruchomieniu nowej wersji aplikacji (blue) rÃ³wnolegle z obecnÄ… wersjÄ… (green).
   - Po przetestowaniu nowej wersji, ruch sieciowy jest przekierowywany do nowej wersji.
   - Kubernetes nie wspiera bezpoÅ›rednio tej strategii, ale moÅ¼na jÄ… zaimplementowaÄ‡ za pomocÄ… dodatkowych narzÄ™dzi i konfiguracji, takich jak Ingress lub Service.

4. **Canary Deployment**:
   - Polega na wdraÅ¼aniu nowej wersji aplikacji tylko dla czÄ™Å›ci uÅ¼ytkownikÃ³w, podczas gdy reszta nadal korzysta z obecnej wersji.
   - Pozwala to na przetestowanie nowej wersji w rzeczywistych warunkach przed peÅ‚nym wdroÅ¼eniem.
   - Podobnie jak Blue-Green Deployment, Kubernetes nie wspiera bezpoÅ›rednio tej strategii, ale moÅ¼na jÄ… zaimplementowaÄ‡ za pomocÄ… dodatkowych narzÄ™dzi i konfiguracji, takich jak Ingress, Service lub narzÄ™dzia do zarzÄ…dzania ruchem.

5. **A/B Testing**:
   - Jest to strategia podobna do Canary Deployment, ale z wiÄ™kszym naciskiem na testowanie rÃ³Å¼nych wariantÃ³w aplikacji w celu okreÅ›lenia, ktÃ³ry z nich dziaÅ‚a lepiej.
   - Wymaga zaawansowanego zarzÄ…dzania ruchem i monitorowania wynikÃ³w, aby porÃ³wnaÄ‡ rÃ³Å¼ne wersje aplikacji.

KaÅ¼da z tych strategii ma swoje zalety i wady, a wybÃ³r odpowiedniej strategii zaleÅ¼y od specyfiki aplikacji, wymagaÅ„ biznesowych oraz tolerancji na przestoje.

### LivenessProbe i ReadinessProbe

W Kubernetes, LivenessProbe i ReadinessProbe to mechanizmy, ktÃ³re pozwalajÄ… na monitorowanie stanu aplikacji uruchomionych w podach.

1. **LivenessProbe**:
   - SÅ‚uÅ¼y do sprawdzania, czy aplikacja dziaÅ‚ajÄ…ca w podzie jest w stanie Å¼ywym (czyli czy dziaÅ‚a poprawnie).
   - JeÅ›li LivenessProbe wykryje, Å¼e aplikacja nie dziaÅ‚a poprawnie, Kubernetes automatycznie zrestartuje pod, aby sprÃ³bowaÄ‡ przywrÃ³ciÄ‡ aplikacjÄ™ do stanu operacyjnego.
   - PrzykÅ‚ad konfiguracji LivenessProbe:
     ```yaml
     livenessProbe:
       httpGet:
         path: /healthz
         port: 8080
       initialDelaySeconds: 3
       periodSeconds: 3
     ```

2. **ReadinessProbe**:
   - SÅ‚uÅ¼y do sprawdzania, czy aplikacja dziaÅ‚ajÄ…ca w podzie jest gotowa do obsÅ‚ugi ruchu sieciowego.
   - JeÅ›li ReadinessProbe wykryje, Å¼e aplikacja nie jest gotowa, pod zostanie oznaczony jako "niedostÄ™pny" i nie bÄ™dzie obsÅ‚ugiwaÅ‚ ruchu sieciowego do momentu, aÅ¼ aplikacja bÄ™dzie gotowa.
   - PrzykÅ‚ad konfiguracji ReadinessProbe:
     ```yaml
     readinessProbe:
       httpGet:
         path: /ready
         port: 8080
       initialDelaySeconds: 3
       periodSeconds: 3
     ```



## Readiness Probe vs Liveness Probe - Tabele PorÃ³wnawcze

### Tabela 1: Podstawowe RÃ³Å¼nice

| Cecha | Readiness Probe | Liveness Probe |
|-------|-----------------|----------------|
| **Pytanie** | Czy jestem gotowy przyjmowaÄ‡ ruch? | Czy jestem Å¼ywy i dziaÅ‚am poprawnie? |
| **Gdy FAIL** | Pod **nie dostaje ruchu** (usuniÄ™ty z Service) | Pod jest **restartowany** przez Kubernetes |
| **Status poda** | Pod dziaÅ‚a, ale jest oznaczony jako NotReady | Pod jest killowany i tworzony na nowo |
| **Kiedy sprawdza** | Od razu po `initialDelaySeconds` | Od razu po `initialDelaySeconds` |
| **CzÄ™stotliwoÅ›Ä‡** | Co `periodSeconds` (np. co 5s) | Co `periodSeconds` (np. co 10s) |
| **WpÅ‚yw na Rolling Update** | âœ… **Blokuje** deployment jeÅ›li FAIL | âŒ **Nie blokuje** - pody siÄ™ restartujÄ… |
| **Ochrona przed zÅ‚ym wdroÅ¼eniem** | âœ… **TAK** - zatrzymuje rollout | âŒ **NIE** - pozwala podom wystartowaÄ‡ |
| **WpÅ‚yw na uÅ¼ytkownikÃ³w** | Zero bÅ‚Ä™dÃ³w - ruch idzie do zdrowych podÃ³w | MoÅ¼liwe bÅ‚Ä™dy podczas oczekiwania na restart |
| **MoÅ¼na wyÅ‚Ä…czyÄ‡ po starcie** | âœ… TAK - pod moÅ¼e staÄ‡ siÄ™ NotReady | âœ… TAK - wymusza restart |
| **Typowy use case** | Aplikacja startuje, Å‚aduje cache, czeka na DB | Wykrywanie deadlockÃ³w, wyciekÃ³w pamiÄ™ci |

### Tabela 2: Scenariusze Deployment (4 repliki)

| Scenariusz | Bez Readiness (tylko Liveness) | Z Readiness + Liveness |
|------------|--------------------------------|------------------------|
| **Nowa wersja aplikacji ma bÅ‚Ä…d 500** | âŒ Nowe pody dostajÄ… ruch przez 30s (initialDelay), uÅ¼ytkownicy dostajÄ… bÅ‚Ä™dy, potem CrashLoopBackOff | âœ… Nowe pody NIE dostajÄ… ruchu, rollout zatrzymany, stare pody dziaÅ‚ajÄ… |
| **Nowa wersja ma bÅ‚Ä…d startowy** | âŒ Pod wystartuje, dostanie ruch, bÄ™dzie sypaÄ‡ bÅ‚Ä™dami, restart po 30s | âœ… Pod wystartuje, NIE dostanie ruchu, rollout zatrzymany |
| **Aplikacja zawiesza siÄ™ po 5 minutach** | âœ… Liveness wykryje po 3x fail i zrestartuje | âœ… Readiness+Liveness: usuniÄ™ty z Service + restart |
| **Aplikacja potrzebuje 60s na start (cache)** | âŒ Dostanie ruch za wczeÅ›nie (jeÅ›li initialDelay < 60s) | âœ… Readiness czeka aÅ¼ aplikacja potwierdzi gotowoÅ›Ä‡ |
| **Deployment nowej wersji** | âŒ Rolling update **kontynuowany** mimo bÅ‚Ä™dÃ³w | âœ… Rolling update **zatrzymany** po pierwszym zÅ‚ym podzie |
| **Stan klastra po zÅ‚ym deploymencie** | CzÄ™Å›Ä‡ podÃ³w w CrashLoopBackOff, czÄ™Å›Ä‡ starych dziaÅ‚a | Wszystkie stare pody dziaÅ‚ajÄ…, nowe pody czekajÄ… |

### Tabela 3: Timeline ZÅ‚ego Deploymentu

| Czas | Tylko Liveness | Readiness + Liveness |
|------|----------------|----------------------|
| **T=0s** | Nowy pod #5 startuje | Nowy pod #5 startuje |
| **T=1s** | âœ… Pod "Ready" (brak Readiness) | â³ Czeka na pierwszÄ… Readiness probe |
| **T=1s** | âŒ Pod dodany do Service | â³ Pod NIE w Service |
| **T=1-30s** | ğŸ’¥ **Users dostajÄ… 500 errors!** | âœ… Ruch idzie do starych podÃ³w |
| **T=5s** | - | âŒ Readiness: FAIL #1 |
| **T=10s** | - | âŒ Readiness: FAIL #2 |
| **T=15s** | - | âŒ Readiness: FAIL #3 â†’ Pod NotReady |
| **T=30s** | âŒ Liveness: FAIL #1 | âŒ Liveness: FAIL #1 |
| **T=40s** | âŒ Liveness: FAIL #2 | âŒ Liveness: FAIL #2 |
| **T=50s** | âŒ Liveness: FAIL #3 â†’ **RESTART** | âŒ Liveness: FAIL #3 â†’ **RESTART** |
| **T=51s** | âœ… Po restarcie znowu "Ready" | â³ Po restarcie czeka na Readiness |
| **T=51-80s** | ğŸ’¥ **Users znowu dostajÄ… bÅ‚Ä™dy!** | âœ… Ruch dalej do starych podÃ³w |
| **T=80s+** | ğŸ” CrashLoopBackOff (z opÃ³Åºnieniem) | ğŸ” CrashLoopBackOff (ale bez wpÅ‚ywu na users) |
| **Stan koÅ„cowy** | âš ï¸ Deployment czÄ™Å›ciowo failed, users mieli downtime | âœ… Deployment zatrzymany, zero downtime |

### Tabela 4: Konfiguracja - Best Practices

| Parametr | Readiness Probe | Liveness Probe | Uzasadnienie |
|----------|-----------------|----------------|---------------|
| **initialDelaySeconds** | 5-10s | 30-60s | Readiness sprawdza wczeÅ›nie; Liveness daje czas na start |
| **periodSeconds** | 5s | 10s | Readiness czÄ™Å›ciej (szybka reakcja na problemy) |
| **timeoutSeconds** | 3s | 5s | Readiness szybsza; Liveness moÅ¼e czekaÄ‡ dÅ‚uÅ¼ej |
| **successThreshold** | 1 | 1 | Pojedyncze potwierdzenie wystarczy |
| **failureThreshold** | 3 | 3 | 3 nieudane prÃ³by = problem (15s dla Readiness, 30s dla Liveness) |
| **Endpoint** | `/ready` lub `/health/ready` | `/health` lub `/healthz` | Osobne endpointy dla rÃ³Å¼nych sprawdzeÅ„ |

### Tabela 5: Rodzaje Probe

| Typ | PrzykÅ‚ad | Kiedy uÅ¼ywaÄ‡ |
|-----|----------|--------------|
| **httpGet** | `path: /ready`<br>`port: 8080` | âœ… REST API, web aplikacje (NAJCZÄ˜ÅšCIEJ) |
| **tcpSocket** | `port: 3306` | âœ… Bazy danych, TCP services (MySQL, Redis) |
| **exec** | `command: ["cat", "/tmp/ready"]` | âœ… Niestandardowe sprawdzenia, legacy apps |
| **grpc** | `port: 9090`<br>`service: myservice` | âœ… gRPC services (K8s 1.24+) |

### Tabela 6: Statusy Poda

| Status | Readiness = PASS | Readiness = FAIL | Liveness = FAIL |
|--------|------------------|------------------|-----------------|
| **Pod Status** | Running | Running | Running â†’ Restart |
| **Ready Condition** | True (1/1) | False (0/1) | - |
| **W Service** | âœ… TAK | âŒ NIE | âœ… TAK (do momentu restartu) |
| **Dostaje ruch** | âœ… TAK | âŒ NIE | âœ… TAK (do momentu restartu) |
| **kubectl get pods** | `myapp-xxx 1/1 Running` | `myapp-xxx 0/1 Running` | `myapp-xxx 0/1 CrashLoopBackOff` |
| **Rollout status** | Progressing | Stuck/Failed | Progressing (ale pody restartujÄ…) |

### Tabela 7: Co sprawdzaÄ‡ w kaÅ¼dej probe?

| Sprawdzenie | Readiness Probe | Liveness Probe |
|-------------|-----------------|----------------|
| **Podstawowe API dziaÅ‚a** | âœ… TAK | âœ… TAK |
| **PoÅ‚Ä…czenie z bazÄ… danych** | âœ… TAK | âŒ NIE* |
| **ZaleÅ¼noÅ›ci zewnÄ™trzne (API, cache)** | âœ… TAK | âŒ NIE* |
| **PamiÄ™Ä‡ dostÄ™pna** | âš ï¸ Opcjonalnie | âœ… TAK |
| **Deadlock detection** | âŒ NIE | âœ… TAK |
| **Cache zaÅ‚adowany** | âœ… TAK | âŒ NIE |
| **Credentials waÅ¼ne** | âœ… TAK | âš ï¸ Opcjonalnie |

\* **Uwaga:** Liveness NIE powinien sprawdzaÄ‡ zaleÅ¼noÅ›ci zewnÄ™trznych, bo jeÅ›li DB padnie, wszystkie pody siÄ™ zrestartujÄ… (co nie pomoÅ¼e).

### Tabela 8: BÅ‚Ä™dy i Konsekwencje

| BÅ‚Ä…d konfiguracji | Konsekwencja | Jak naprawiÄ‡ |
|-------------------|--------------|--------------|
| Brak Readiness Probe | ZÅ‚e pody dostajÄ… ruch podczas deploymentu | Dodaj Readiness: httpGet /ready |
| Liveness = Readiness (ten sam endpoint) | Podczas przeciÄ…Å¼enia pody siÄ™ restartujÄ… | UÅ¼yj osobnych endpointÃ³w |
| Za krÃ³tki initialDelaySeconds | Pody failujÄ… przed startem aplikacji | ZwiÄ™ksz do czasu startu +10s |
| Za dÅ‚ugi initialDelaySeconds | Wolny rollout, opÃ³Åºnione wykrycie problemÃ³w | Zmniejsz, uÅ¼yj startupProbe |
| Zbyt agresywny failureThreshold=1 | FaÅ‚szywe alarmy, niepotrzebne restarty | Ustaw na 3 (lub wiÄ™cej) |
| Liveness sprawdza DB | Jak DB padnie, wszystkie pody siÄ™ restartujÄ… | Liveness = tylko stan aplikacji |
| Brak timeout | Pody wiszÄ… w nieskoÅ„czonoÅ›Ä‡ | Ustaw timeoutSeconds: 3-5s |

### Tabela 9: Komendy diagnostyczne

| Co sprawdziÄ‡ | Komenda | Co pokazuje |
|--------------|---------|-------------|
| Status podÃ³w | `kubectl get pods` | Ready (1/1) vs NotReady (0/1) |
| SzczegÃ³Å‚y probe | `kubectl describe pod <name>` | Historia Readiness/Liveness events |
| Dlaczego NotReady | `kubectl describe pod <name> \| grep -A 10 Conditions` | Readiness failed reason |
| Logi aplikacji | `kubectl logs <pod>` | BÅ‚Ä™dy aplikacji |
| Rollout status | `kubectl rollout status deployment/<name>` | Czy deployment progresuje |
| Events w czasie | `kubectl get events --sort-by=.metadata.creationTimestamp` | Timeline co siÄ™ dziaÅ‚o |
| Probes config | `kubectl get pod <name> -o yaml \| grep -A 15 Probe` | Aktualna konfiguracja probe |